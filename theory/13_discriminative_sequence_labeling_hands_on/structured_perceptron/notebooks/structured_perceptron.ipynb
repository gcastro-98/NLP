{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:48.896179Z",
     "start_time": "2020-05-12T14:44:48.649812Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:49.049052Z",
     "start_time": "2020-05-12T14:44:48.998738Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import scipy\n",
    "import numpy as np\n",
    "import os,sys\n",
    "\n",
    "currentdir = Path.cwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:49.173923Z",
     "start_time": "2020-05-12T14:44:49.148685Z"
    }
   },
   "outputs": [],
   "source": [
    "from skseq.sequences import sequence\n",
    "from skseq.sequences.sequence import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Sequence Object\n",
    "\n",
    "- Sequence objects are defined in ``skseq/sequences/sequence.py``. \n",
    "    - A sequence in a supervised learning problem consist on a set of words and tags associated to words.\n",
    "    - For example ``w_1/t_1 w_2/t_2 w_3/t_3`` is a sequence of length 3 with words ``w_i`` and tags ``t_i``.\n",
    "\n",
    "\n",
    "In order to instanciate a Sequence we essentially need a list of words and a list of tags of the same size. In order to do it efficiently we will not store strings for the words and tags. We will store an integer values that will represent words and tags.\n",
    "\n",
    "- **.x** attribute: list of words (integer words)\n",
    "\n",
    "- **.y** attribute: list of tags (integer tags)\n",
    "\n",
    "Then we need to keep a mapping from integers to words and from integers to tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:49.536872Z",
     "start_time": "2020-05-12T14:44:49.521141Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = Sequence(x=[\"my\",\"sequence\",\"is\",\"cool\"], y=[0,2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:49.719246Z",
     "start_time": "2020-05-12T14:44:49.701644Z"
    }
   },
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:49.908275Z",
     "start_time": "2020-05-12T14:44:49.892340Z"
    }
   },
   "outputs": [],
   "source": [
    "seq.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:50.089167Z",
     "start_time": "2020-05-12T14:44:50.072251Z"
    }
   },
   "outputs": [],
   "source": [
    "seq.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a vocabulary and a SequenceList\n",
    "\n",
    "Given a training set with words and tags we want to build a SequenceList object definded in  ``skseq/sequences/sequence_list.py``.\n",
    "\n",
    "A  SequenceList is a class that is initialized using a\n",
    "- dictionary for the words\n",
    "- a dictionary for the tags\n",
    "- an empty sequence list where the Sequences read from the data will be stored.\n",
    "\n",
    "\n",
    "    class SequenceList(object):\n",
    "\n",
    "        def __init__(self, x_dict, y_dict):\n",
    "            self.x_dict = x_dict\n",
    "            self.y_dict = y_dict\n",
    "            self.seq_list = []\n",
    "\n",
    "\n",
    "Let us create 3 sequence list for train, test and validation.  \n",
    "\n",
    "We will use the conll dataset and the class  ``PostagCorpus``.\n",
    "The class has a method ``.read_sequence_list_conll`` that will return the **SequenceList** object we want\n",
    "\n",
    "\n",
    "\n",
    "    def read_sequence_list_conll(self, train_file,\n",
    "                                 mapping_file=(\"%s/en-ptb.map\"\n",
    "                                               % dirname(__file__)),\n",
    "                                 max_sent_len=100000,\n",
    "                                 max_nr_sent=100000):\n",
    "\n",
    "        # Build mapping of postags:\n",
    "        mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            for line in open(mapping_file):\n",
    "                coarse, fine = line.strip().split(\"\\t\")\n",
    "                mapping[coarse.lower()] = fine.lower()\n",
    "\n",
    "        instance_list = self.read_conll_instances(train_file,\n",
    "                                                  max_sent_len,\n",
    "                                                  max_nr_sent,\n",
    "                                                  mapping)\n",
    "\n",
    "        seq_list = SequenceList(self.word_dict, self.tag_dict)\n",
    "\n",
    "        for sent_x, sent_y in instance_list:\n",
    "            seq_list.add_sequence(sent_x, sent_y,  self.word_dict, self.tag_dict)\n",
    "\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:50.467685Z",
     "start_time": "2020-05-12T14:44:50.443690Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq.readers.pos_corpus\n",
    "corpus = skseq.readers.pos_corpus.PostagCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:51.531245Z",
     "start_time": "2020-05-12T14:44:50.839950Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/conll\"\n",
    "\n",
    "data_path = parentdir + data_path\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"/test-23.conll\",\n",
    "                                           max_sent_len=100, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"/dev-22.conll\", \n",
    "                                          max_sent_len=100, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:53.973987Z",
     "start_time": "2020-05-12T14:44:53.957066Z"
    }
   },
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:44:58.288791Z",
     "start_time": "2020-05-12T14:44:58.273091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:00.317558Z",
     "start_time": "2020-05-12T14:45:00.298808Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:00.517435Z",
     "start_time": "2020-05-12T14:45:00.501413Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(train_seq))\n",
    "print(type(train_seq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:00.787327Z",
     "start_time": "2020-05-12T14:45:00.771264Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:01.112817Z",
     "start_time": "2020-05-12T14:45:01.097104Z"
    }
   },
   "outputs": [],
   "source": [
    "# first sentence\n",
    "train_seq[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:01.282537Z",
     "start_time": "2020-05-12T14:45:01.266328Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:01.473899Z",
     "start_time": "2020-05-12T14:45:01.457981Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:01.666191Z",
     "start_time": "2020-05-12T14:45:01.648613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set of possible tags Lambda\n",
    "# train_seq.y_dict is a dictionary of mappings from tag to integer id \n",
    "train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:01.866931Z",
     "start_time": "2020-05-12T14:45:01.851802Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:02.072279Z",
     "start_time": "2020-05-12T14:45:02.056965Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[1].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:02.273283Z",
     "start_time": "2020-05-12T14:45:02.257605Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[1].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:02.497471Z",
     "start_time": "2020-05-12T14:45:02.480973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping from word to integer\n",
    "c =0\n",
    "print(\"First 5 word:id pairs in the dictionary\\n\")\n",
    "for i in train_seq.x_dict:\n",
    "    print(i,\":\", train_seq.x_dict[i])\n",
    "    c +=1\n",
    "    if c>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our corpus ``sequencelist`` to map integers to words\n",
    "\n",
    "Sequences can use ``SequenceList`` objects to map word_ids and tag_ids to words and tags.\n",
    "\n",
    "All ``sequence`` objects have the **``.to_words``** method which allows us to print the words given a **``SequenceList``** object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:03.080449Z",
     "start_time": "2020-05-12T14:45:03.065522Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:03.283031Z",
     "start_time": "2020-05-12T14:45:03.268376Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence = train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:03.463955Z",
     "start_time": "2020-05-12T14:45:03.448529Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence.to_words(sequence_list=train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to (linear) discriminative sequence models\n",
    "\n",
    "Discriminative sequence models aim to solve the following:\n",
    "\n",
    "$$\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ P(Y=y\\,|\\,X=x)=\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \\boldsymbol{w}\\cdot\\boldsymbol{f}(x, y)$$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the model's weight vector, and $\\boldsymbol{f}(x, y)$ is a feature vector. Notice that now both $y$ and $x$ are $N$-dimensional vectors.\n",
    "\n",
    "The sequence score is computed as the product of the weights with the feature vector:\n",
    "\n",
    "| Score &nbsp; &nbsp; &nbsp; &nbsp;| Hidden Markov Models| Discriminative Models &nbsp; &nbsp; &nbsp;  |\n",
    "| ----- | ------------------- | ---------------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}$ | $\\log P(x_i\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i)$ |\n",
    "| $\\textrm{score}_\\textrm{init}$ | $\\log P(y_1\\,|\\,\\mathrm{start})$ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}$ | $\\log P(y_{i+1}\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}$ | $\\log P(\\mathrm{stop}\\,|\\,y_N)$ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N)$ |\n",
    "\n",
    "Notice that the scores computed using the feature vector depend on two sequential values of the output variable, $y$, but may depend on the whole observed input, $x$. We can now rewrite the above expression as\n",
    "\n",
    "$$\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N) = \n",
    "\\\\\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\textrm{score}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\textrm{score}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\textrm{score}_\\textrm{trans}(i, x, y_i, y_{i+1}) +\n",
    "\\textrm{score}_\\textrm{final}(x, y_N)\n",
    "$$\n",
    "\n",
    "The reader can notice that feature vectors depend locally on the output variable. The features depend on \n",
    "\n",
    "- a single $y_i$ in the case of emission scores, initial scores and final scores.\n",
    "- or a pair  $y_i, y_{i+1}$ in the case of transition scores. \n",
    "\n",
    "### Features\n",
    "\n",
    "We will use two types of simple features: \n",
    "\n",
    "- **Features that mimic the features used by the HMM**\n",
    "    - They will allow us to directly compare the performance of a generative vs a discriminative approach.\n",
    "    \n",
    "\n",
    "- **Features that are implicit in the HMM**\n",
    "    - They are simple indicators of the initial, transition, final and emission events.\n",
    "    - Given a certain position $i$ and state $c$, the set of features that mimic the HMM are:\n",
    "\n",
    "| Conditions to be met       |    Name             |\n",
    "| ----------------           | ----------------    |\n",
    "| $y_i=c  \\,\\, \\& \\,\\, i =0$        | Initial features    |\n",
    "| $y_i=c   \\,\\, \\& \\,\\, y_{i-1}=c$  | Transition features |\n",
    "| $y_i=c_k \\,\\, \\& \\,\\,  i=N$        | Final features      |\n",
    "| $x_i=w_j \\,\\, \\& \\,\\,  y_i=c_k$    | Emission features   |\n",
    "\n",
    "When we used a generative model we were forced to make some independence assumptions. However, since we have adopted a discriminative approach,where we model $P(Y | X)$ rather than $P(X,Y)$ we are not tied anymore to some of these assumptions. In particular:\n",
    "\n",
    "- We may use “overlapping” features, e.g., features that fire simultaneously for many instances. For example, we can use a feature for a word, such as a feature which fires for the word ”brilliantly”, and another for prefixes and suffixes of that word, such as one which fires if the last two letters of the word are ”ly”. This would lead to an awkward model if we wanted to insist on a generative approach.\n",
    "\n",
    "\n",
    "- We may use features that depend arbitrarily on the entire input sequence $x$. On the other hand, we still need to resort to “local” features with respect to the outputs (e.g. looking only at consecutive state pairs), otherwise decoding algorithms will become more expensive.\n",
    "\n",
    "\n",
    "#### Typical features used for POS tagging with discriminative models\n",
    "\n",
    "The following table shows some typical POS tagging features. Let us consider $P_{set}$ and $S_{set}$ to be two sets of prefixes and sufixes respectively (set by the user).\n",
    "\n",
    "\n",
    "| Conditions to be met for some of the most typical POS features     |    Name      |\n",
    "| ----------------                                | ----------------    |\n",
    "| $y_i=c , \\,\\,  i =0$                      | Initial features    |\n",
    "| $y_i=c ,\\,\\,  y_{i-1}=c$                | Transition features |\n",
    "| $y_i=c_k ,\\,\\, i=N$                     | Final features      |\n",
    "| $x_i=w_j ,\\,\\,  y_i=c_k$                 | Basic Emission features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ is uppercased } ,\\,\\,  y_i=c_k$                 | Upper case features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains digit} ,\\,\\,  y_i=c_k$                 | Digit features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains hyphen} ,\\,\\,  y_i=c_k$                 | Hyphen features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[0:i] \\in P_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Prefix features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[-i] \\in S_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Suffix features|\n",
    "\n",
    "We can have more complex features which look arbitrarily to the input sequence. We are not going to have them in this exercise only for performance reasons (to have less features and smaller caches). State-of-the-art sequence classifiers can easily reach over one million features!\n",
    "\n",
    "Our features subdivide in two groups\n",
    "\n",
    "- **node features**: $f_{\\text{emiss}}, f_{\\text{init}}, f_{\\text{final}}$. Node features depend only on a single position in the state sequence (or node in the trellis).\n",
    "    \n",
    "    \n",
    "- **edge features**: $f_{\\text{trans}}$. Edge features depend on two consecutive positions in the state sequence (an edge in the trellis)\n",
    "\n",
    "\n",
    "    \n",
    "| score  definitions: scalar product between features and weights |\n",
    "| ------------------------------------------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}\\,(i,x,y_1) =\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}\\,(i, x, y_i)$ |\n",
    "|$\\textrm{score}_\\textrm{init}(x,y_1)=\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init} \\,(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}\\,(i,x,y_i,y_{i+1}) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}\\,(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}\\,(x,y_N) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}\\,(x, y_N)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Decoding\n",
    "\n",
    "One important thing to notice is that the decoding process - the process by which we pick the most likely label $y_i$ for the observation $x_i$ - stays the same as in standard HMMs. This means **we do not need to develop new decoders, as long as we have existing decoders for HMMs**. Instead, we only need new functions to compute the scores. Because of this, we will keep using the Viterbi and Forward-Backward algorithms.\n",
    "\n",
    "\n",
    "For decoding,   there are three important problems that need to be solved:\n",
    "\n",
    "1. Given $X=x$, compute the most likely output sequence $\\hat{y}$ (the one which maximizes $P_{w}(Y=y|X=x)$). \n",
    "2. Compute the posterior marginals $P_{w}(Y_i=y_i|X=x)$ at each position $i$.\n",
    "3. Evaluate the partition function $Z(w,x)$. \n",
    "\n",
    "Interestingly, all these problems can be solved by using the very same\n",
    "algorithms that were \n",
    "already implemented for HMMs: the Viterbi algorithm (for 1) and the forward-backward algorithm (for 2--3). All that changes is the way the scores are computed. \n",
    "\n",
    "\n",
    "\n",
    "### Training the classifier\n",
    "\n",
    "Today we will cover two different approaches to training sequential discriminative models. Given a training set with $M$ observation-label pairs, $\\{(x_m, y_m)\\}_{m=1}^M$ (note that $x_m$, $y_m$ are $N$-dimensional vectors, as $m$ indexes the training sample):\n",
    "\n",
    "* **Structured Perceptron** iteratively updates $w$ in order to correctly classify the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training  a POS tagger in english\n",
    "\n",
    "## Loading the data: the conll dataset for part of speech tagging\n",
    "\n",
    "\n",
    "\n",
    "#### Example of two sequences in the dataset\n",
    "\n",
    "    1\tMs.\t_\tNN\tNNP\t_\t2\tNMOD\t_\t_\n",
    "    2\tHaag\t_\tNN\tNNP\t_\t3\tSUB\t_\t_\n",
    "    3\tplays\t_\tVB\tVBZ\t_\t0\tROOT\t_\t_\n",
    "    4\tElianti\t_\tNN\tNNP\t_\t3\tOBJ\t_\t_\n",
    "    5\t.\t_\t.\t.\t_\t3\tP\t_\t_\n",
    "\n",
    "    1\tRolls-Royce\t_\tNN\tNNP\t_\t4\tNMOD\t_\t_\n",
    "    2\tMotor\t_\tNN\tNNP\t_\t4\tNMOD\t_\t_\n",
    "    3\tCars\t_\tNN\tNNPS\t_\t4\tNMOD\t_\t_\n",
    "    4\tInc.\t_\tNN\tNNP\t_\t5\tSUB\t_\t_\n",
    "    5\tsaid\t_\tVB\tVBD\t_\t0\tROOT\t_\t_\n",
    "    6\tit\t_\tPR\tPRP\t_\t7\tSUB\t_\t_\n",
    "    7\texpects\t_\tVB\tVBZ\t_\t5\tVMOD\t_\t_\n",
    "    8\tits\t_\t.\tPRP$\t_\t10\tNMOD\t_\t_\n",
    "    9\tU.S.\t_\tNN\tNNP\t_\t10\tNMOD\t_\t_\n",
    "    10\tsales\t_\tNN\tNNS\t_\t12\tSUB\t_\t_\n",
    "    11\tto\t_\tTO\tTO\t_\t12\tVMOD\t_\t_\n",
    "    12\tremain\t_\tVB\tVB\t_\t7\tVMOD\t_\t_\n",
    "    13\tsteady\t_\tJJ\tJJ\t_\t12\tPRD\t_\t_\n",
    "    14\tat\t_\tIN\tIN\t_\t12\tVMOD\t_\t_\n",
    "    15\tabout\t_\tIN\tIN\t_\t17\tNMOD\t_\t_\n",
    "    16\t1,200\t_\tCD\tCD\t_\t15\tAMOD\t_\t_\n",
    "    17\tcars\t_\tNN\tNNS\t_\t14\tPMOD\t_\t_\n",
    "    18\tin\t_\tIN\tIN\t_\t12\tVMOD\t_\t_\n",
    "    19\t1990\t_\tCD\tCD\t_\t18\tPMOD\t_\t_\n",
    "    20\t.\t_\t.\t.\t_\t5\tP\t_\t_\n",
    "    \n",
    "\n",
    "#### corpus.read_sequence_list_conll\n",
    "\n",
    "This method will read the data. Each phrase in the dataset will become a **Sequence** which will be appended to the SequenceList\n",
    "\n",
    "skseq.sequences.sequence_list.SequenceList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:04.700572Z",
     "start_time": "2020-05-12T14:45:04.685579Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is completly unrealistic, words in train and test will be different usually\n",
    "len(train_seq.x_dict),len(test_seq.x_dict), len(dev_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:04.999155Z",
     "start_time": "2020-05-12T14:45:04.979427Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_seq.y_dict),len(test_seq.y_dict), len(dev_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.y_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Mapper (or feature template)\n",
    "\n",
    "Given a dataset, first we will build a \n",
    "\n",
    "-  **`SequenceList`** with the dataset.\n",
    "\n",
    "in order to build the features from the instantiated  **`SequenceList`** we will use\n",
    "\n",
    "- An instance from **`IDFeatures`** (we will call it `feature_mapper`) must be instantiated.\n",
    "- **`feature_mapper.build_features()`** must be executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating features from data\n",
    "\n",
    "\n",
    "### Instantiating a feature_mapper\n",
    "We will assume `feature_mapper` has been instantiated with\n",
    "```\n",
    "    feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "```\n",
    "\n",
    "**The IDFeatures object will be referred to as a  `feature_mapper`.**\n",
    "\n",
    "\n",
    "#### About feature_mappers\n",
    "A `feature_mapper` will contain the following attributes:\n",
    "\n",
    "- the dataset in `.dataset`\n",
    "    - if we instantiate the feature mapper with a dataset X then `feature_mapper.dataset` will be a copy of X\n",
    "\n",
    "- a dictionary of features in `.feature_dict`\n",
    "    - this dictionary will default to `{}`. \n",
    "    - In order to build the features the feature mapper must call `.build_features()` function.\n",
    "    \n",
    "- a list of features in `.feature_list`\n",
    "    - this list will default to `[]`. \n",
    "    - in order to build the list of features the feature mapper must call `.build_features()` function.\n",
    "\n",
    "A `feature_mapper` will contain a method to generate features, `.build_features`:\n",
    "- this method will create features using the `.dataset`.\n",
    "- this method will also fill `.feature_dict` and `.feature_list`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:05.669660Z",
     "start_time": "2020-05-12T14:45:05.653478Z"
    }
   },
   "outputs": [],
   "source": [
    "type(train_seq), type(train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:06.545377Z",
     "start_time": "2020-05-12T14:45:06.531156Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "\n",
    "# You can improve the results adding more features\n",
    "#from skseq.sequences import extended_feature\n",
    "#feature_mapper = skseq.sequences.extended_feature.ExtendedFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:06.806189Z",
     "start_time": "2020-05-12T14:45:06.790109Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:07.215940Z",
     "start_time": "2020-05-12T14:45:07.199038Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling ```feature_mapper.build_features()```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:08.218260Z",
     "start_time": "2020-05-12T14:45:07.711913Z"
    }
   },
   "outputs": [],
   "source": [
    "# get features\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:08.235748Z",
     "start_time": "2020-05-12T14:45:08.220056Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(list(feature_mapper.__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:08.762522Z",
     "start_time": "2020-05-12T14:45:08.747275Z"
    }
   },
   "outputs": [],
   "source": [
    "len(feature_mapper.feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:09.395466Z",
     "start_time": "2020-05-12T14:45:09.380025Z"
    }
   },
   "outputs": [],
   "source": [
    "len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:10.057025Z",
     "start_time": "2020-05-12T14:45:10.040900Z"
    }
   },
   "outputs": [],
   "source": [
    "list(feature_mapper.feature_dict)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:12.015629Z",
     "start_time": "2020-05-12T14:45:11.998975Z"
    }
   },
   "outputs": [],
   "source": [
    "list(feature_mapper.feature_dict)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(feature_mapper.feature_dict.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:12.198829Z",
     "start_time": "2020-05-12T14:45:12.178526Z"
    }
   },
   "outputs": [],
   "source": [
    "set([x.split(\":\")[0] for x in feature_mapper.feature_dict.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving features from a `feature_mapper`\n",
    "\n",
    "we can get the features of a given sequence using **``feature_mapper.get_sequence_features``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:12.914187Z",
     "start_time": "2020-05-12T14:45:12.898804Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.feature_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:13.566771Z",
     "start_time": "2020-05-12T14:45:13.550785Z"
    }
   },
   "outputs": [],
   "source": [
    "len(feature_mapper.feature_dict),len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:13.904260Z",
     "start_time": "2020-05-12T14:45:13.888606Z"
    }
   },
   "outputs": [],
   "source": [
    "# for any position i, this will be len 4, corresponding to \n",
    "# initial,emission, transition, and final features\n",
    "m = 1\n",
    "len(feature_mapper.feature_list[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:14.285559Z",
     "start_time": "2020-05-12T14:45:14.271063Z"
    }
   },
   "outputs": [],
   "source": [
    "id_seq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:14.943547Z",
     "start_time": "2020-05-12T14:45:14.928548Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.dataset[id_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:15.164797Z",
     "start_time": "2020-05-12T14:45:15.148906Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of the sequence\n",
    "len(feature_mapper.dataset[id_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:15.345610Z",
     "start_time": "2020-05-12T14:45:15.328942Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of the types of features\n",
    "len(feature_mapper.feature_list[id_seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding features in a feature mapper\n",
    "\n",
    "\n",
    "### What are all those numbers?\n",
    "\n",
    "Notice that ```feature_mapper.feature_list[id_seq]``` is a list of lists of length 4.\n",
    "\n",
    "This lenght is the same no matter ```id_seq```\n",
    "\n",
    "### Coding of the features\n",
    "\n",
    "All features are saved in **``feature_mapper.feature_dict``**.\n",
    "\n",
    "- **If it is our feature vector why it's not a vector? Good point!** \n",
    "    - In order to make the algorithm \"fast\", the code is written using dicts, so if we access only a few positions from the dict and compute substractions it will be much faster than computing the substraction of two huge weight vectors.\n",
    "\n",
    "Some features are identifyed by starting with **init_tag:**, **prev_tag:**,  **final_prev_tag:**, **id:**\n",
    "\n",
    "- **init_tag:** when they are Initial features\n",
    "    - Example: **``init_tag:noun``** is an initial feature that describes that the first word is a noun\n",
    "    \n",
    "    \n",
    "- **prev_tag:** when they are transition features\n",
    "    - Example: **``prev_tag:noun::noun``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a noun.\n",
    "    - Example: **``prev_tag:noun:.``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a `.` (this is usually foud as the last transition feature since most phrases will end up with a dot)\n",
    "      \n",
    "\n",
    "\n",
    "- **final_prev_tag:** when they are final features\n",
    "    - Example: **``final_prev_tag:.``** is a final feature stating that the last \"word\" in the sentence was a dot.\n",
    "\n",
    "\n",
    "- **id:** when they are emission features\n",
    "    - Example: **``id:plays::verb``** is an emission feature, describing that the current word is plays and the current hidden state is a verb.\n",
    "    - Example: **``id:Feb.::noun``** is an emission feature, describing that the current word is \"Feb.\" and the current hidden state is a noun.\n",
    "    \n",
    "Other features are identified by starting with **uppercased**, **suffix**, **preffix** etc...\n",
    "\n",
    "- **uppercased:** when they contain the current word with an uppercased letter\n",
    "    - Example: **``uppercased::noun``** is a feature stating that current word is uppercased and the current tag is a noun.\n",
    "\n",
    "- **prefix:** when the current word contains a certain prefix.\n",
    "    - Example: prefix:Eli::noun\n",
    "\n",
    "- **suffix:** when the current word contains a certain suffix.\n",
    "    - Example: suffix:ing:verb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:03.848782Z",
     "start_time": "2020-05-12T14:48:03.834004Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Initial features:\",     feature_mapper.feature_list[id_seq][0])\n",
    "print (\"Transition features:\",  feature_mapper.feature_list[id_seq][1])\n",
    "print (\"Final features:\",       feature_mapper.feature_list[id_seq][2])\n",
    "print (\"Emission features:\",    feature_mapper.feature_list[id_seq][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:05.143570Z",
     "start_time": "2020-05-12T14:48:05.127456Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_feature_dict = {word: pos for pos, word in feature_mapper.feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "features are coded as integers, using the `inv_feature_dict` we can see its interpretation.\n",
    "\n",
    "Features have been assigned to a unique string that gives some hint on what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:05.647494Z",
     "start_time": "2020-05-12T14:48:05.633508Z"
    }
   },
   "outputs": [],
   "source": [
    "id_seq = 6\n",
    "seq = train_seq[id_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:05.875160Z",
     "start_time": "2020-05-12T14:48:05.860512Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq[id_seq].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:06.234024Z",
     "start_time": "2020-05-12T14:48:06.219397Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.feature_list[id_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:06.685949Z",
     "start_time": "2020-05-12T14:48:06.669191Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_type = [\"Initial features\", \"Transition features\", \"Final features\", \"Emission features\"]\n",
    "\n",
    "def show_feats(feature_mapper, seq):\n",
    "    for feat,feat_ids in enumerate(feature_mapper.get_sequence_features(seq)):\n",
    "        print(feature_type[feat])\n",
    "        for id_list in feat_ids:\n",
    "            print (\"\\t\",id_list)\n",
    "            for k,id_val in enumerate(id_list):\n",
    "                print (\"\\t\\t\", inv_feature_dict[id_val] )\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feats(feature_mapper, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given an input sequence, how to compute features that are activated\n",
    "\n",
    "We have stored all features in  **``feature_mapper.feature_dict``**.\n",
    "\n",
    "Now how can we know when a particular word and tag at a particular position fire any of the created features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:07.312917Z",
     "start_time": "2020-05-12T14:48:07.298331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at some features and the position they have assigned\n",
    "c =0\n",
    "print(\"First 5 features in the dicitionary\\n\")\n",
    "for i in feature_mapper.feature_dict:\n",
    "    print(i, \":\", feature_mapper.feature_dict[i])\n",
    "    c +=1\n",
    "    if c>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:07.538319Z",
     "start_time": "2020-05-12T14:48:07.524019Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence = train_seq[1]\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activated Features\n",
    "\n",
    "inside the feature mapper there is the **``get_initial_features``** method\n",
    "\n",
    "    def get_initial_features(self, sequence, y):\n",
    "        if y not in self.initial_state_feature_cache:\n",
    "            edge_idx = []\n",
    "            edge_idx = self.add_initial_features(sequence, y, edge_idx)\n",
    "            self.initial_state_feature_cache[y] = edge_idx\n",
    "        return self.initial_state_feature_cache[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:07.910441Z",
     "start_time": "2020-05-12T14:48:07.895954Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:08.100229Z",
     "start_time": "2020-05-12T14:48:08.085889Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.get_initial_features(sequence, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:08.269948Z",
     "start_time": "2020-05-12T14:48:08.254830Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:08.463650Z",
     "start_time": "2020-05-12T14:48:08.448781Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:08.629747Z",
     "start_time": "2020-05-12T14:48:08.615251Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:08.812521Z",
     "start_time": "2020-05-12T14:48:08.798085Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_feature_dict[1695]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Structured perceptron\n",
    "\n",
    "In order to train a structured perceptron we need to construct a feature mapper that will translate Sequence objects to numerical features. Then the structured perceptron can be instantiated using\n",
    "\n",
    "- The corpus dictionary of words\n",
    "- The corpus dictionary of tags\n",
    "- The feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:10.205897Z",
     "start_time": "2020-05-12T14:48:09.707903Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "#import skseq.sequences.extended_feature as exfc\n",
    "#feature_mapper = exfc.ExtendedFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:10.222172Z",
     "start_time": "2020-05-12T14:48:10.207634Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus.sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:11.296067Z",
     "start_time": "2020-05-12T14:48:11.281457Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:11.533590Z",
     "start_time": "2020-05-12T14:48:11.506332Z"
    }
   },
   "outputs": [],
   "source": [
    "import skseq.sequences.structured_perceptron as spc\n",
    "\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:11.680863Z",
     "start_time": "2020-05-12T14:48:11.666244Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.state_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:12.589306Z",
     "start_time": "2020-05-12T14:48:12.575004Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.get_num_states(), sp.get_num_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:12.801186Z",
     "start_time": "2020-05-12T14:48:12.786664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_mapper.get_num_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the weights of the perceptron\n",
    "\n",
    "The perceptron starts with all weights set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:14.006767Z",
     "start_time": "2020-05-12T14:48:13.992036Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:14.933956Z",
     "start_time": "2020-05-12T14:48:14.915869Z"
    }
   },
   "outputs": [],
   "source": [
    "len(sp.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:15.807491Z",
     "start_time": "2020-05-12T14:48:15.792636Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.parameters.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions made by the structured Perceptron\n",
    "\n",
    "We can use the method **``.viterbi_decode``** from the structured perceptron to generate a sequence of predictions for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:16.233002Z",
     "start_time": "2020-05-12T14:48:16.219585Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = train_seq[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:16.516096Z",
     "start_time": "2020-05-12T14:48:16.501716Z"
    }
   },
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:16.808226Z",
     "start_time": "2020-05-12T14:48:16.793920Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.get_num_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:17.058774Z",
     "start_time": "2020-05-12T14:48:17.041337Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.viterbi_decode(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a structured perceptron\n",
    "\n",
    "\n",
    "In order to train a structured perceptron we need to construct a feature mapper that will translate Sequence objects to numerical features. Then the structured perceptron can be instantiated using\n",
    "\n",
    "- The corpus dictionary of words\n",
    "- The corpus dictionary of tags\n",
    "- The feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:18.267199Z",
     "start_time": "2020-05-12T14:48:17.718472Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:18.291720Z",
     "start_time": "2020-05-12T14:48:18.269072Z"
    }
   },
   "outputs": [],
   "source": [
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:18.308642Z",
     "start_time": "2020-05-12T14:48:18.293579Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.get_num_states(), sp.get_num_observations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy before training\n",
    "\n",
    "We can use the methods\n",
    "\n",
    "- **``viterbi_decode_corpus``** to generate a list of sequences containing the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:19.398660Z",
     "start_time": "2020-05-12T14:48:19.383522Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_corpus(sequences, sequences_predictions):\n",
    "    \"\"\"Evaluate classification accuracy at corpus level, comparing with\n",
    "    gold standard.\"\"\"\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        pred = sequences_predictions[i]\n",
    "        for j, y_hat in enumerate(pred.y):\n",
    "            if sequence.y[j] == y_hat:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:47.675885Z",
     "start_time": "2020-05-12T14:48:20.326824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev   = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test  = sp.viterbi_decode_corpus(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:47.733798Z",
     "start_time": "2020-05-12T14:48:47.677404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(train_seq.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(dev_seq.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(test_seq.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the structured perceptron\n",
    "\n",
    "\n",
    "The structured perceptron has the method **train_supervised** which allow us to train the weights of the model\n",
    "\n",
    "- **train_supervised**: receives a **SequenceList** object\n",
    "\n",
    "Let us recall that a **SequenceList**  is a list of **Sequence** objects.\n",
    "\n",
    "- Each **Sequence** has the **.x** and **.y** atribute which are the words and tags of the **Sequence** respectively.\n",
    "\n",
    "    - For example, an example of **Sequence** in our train data, train_seq[1], is \n",
    "        - Ms./noun Haag/noun plays/verb Elianti/noun ./. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:47.751480Z",
     "start_time": "2020-05-12T14:48:47.735782Z"
    }
   },
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:53:57.229276Z",
     "start_time": "2020-05-12T14:49:05.709531Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_epochs = 15\n",
    "sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:55:45.341577Z",
     "start_time": "2020-05-12T14:55:45.327204Z"
    }
   },
   "outputs": [],
   "source": [
    "len(sp.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:55:46.173690Z",
     "start_time": "2020-05-12T14:55:46.159267Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:55:46.427511Z",
     "start_time": "2020-05-12T14:55:46.406642Z"
    }
   },
   "outputs": [],
   "source": [
    "sp.save_model(\"perceptron_15_iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2.load_model(dir=\"perceptron_15_iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(train_seq.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(dev_seq.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(test_seq.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the structured perceptron\n",
    "\n",
    "In order to make a tag prediction for a given sequence we can use the **``.viterbi_decode``** method\n",
    "\n",
    "### About  **``.viterbi_decode``**\n",
    "\n",
    "- Compute scores given the observation sequence\n",
    "- Run the forward algorithm and therefore gets\n",
    "    - the predicted sequence of states **``best_states``**\n",
    "    - the total score\n",
    "- Creates a new sequence named **``predicted_sequence``**\n",
    "    - copyes the sequence of words from the input\n",
    "    - assigns the tags from  **``best_states``** to the created sequence\n",
    "    \n",
    "Returns **``predicted_sequence``** as well sas the **``total_score``**\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = train_seq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = sp.viterbi_decode(sequence)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence containing the original words and the tag prediction\n",
    "aux[0].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a given unseen string sequence\n",
    "\n",
    "Let us assume we have the phrase \"David had been asked to write a challenging program for Maria .\"\n",
    "\n",
    "We have to:\n",
    "\n",
    "- convert our string to a ``vlex_seq2.sequences.sequence.Sequence`` object.\n",
    "- use the ``perceptron.viterbi_decode`` method with the previously built ``Sequence`` object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"David had been asked to write a challenging program for Maria .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids  = [train_seq.x_dict[w] for w in p.split()]\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = skseq.sequences.sequence.Sequence(x=word_ids, y=[0 for w in word_ids])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.viterbi_decode(seq)[0].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features correspond to the activated features for the given \n",
    "# list of tags and list of words\n",
    "feature_mapper.get_sequence_features(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feats(feature_mapper, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen words in the corpus\n",
    "An obvious question that might arise is **what happens when a word is not in x_dict** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"there are\", len(train_seq.x_dict), \"words in the dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"David had been asked to write a challenging program for Angel .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should fail because \"Angel\" was never seen\n",
    "word_ids  = [train_seq.x_dict[w] for w in p.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a seq object with empty tags and use it\n",
    "new_seq = skseq.sequences.sequence.Sequence(x=p.split(), y=[int(0) for w in p.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper.get_sequence_features(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.viterbi_decode(new_seq)[0].to_words(train_seq,\n",
    "                                       only_tag_translation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"Sara had been asked to write a challenging program for Angel .\"\n",
    "new_seq = skseq.sequences.sequence.Sequence(x=p.split(), y=[int(0) for w in p.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.viterbi_decode(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.viterbi_decode(new_seq)[0].to_words(train_seq,\n",
    "                                       only_tag_translation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Adding new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ub_nlp]",
   "language": "python",
   "name": "conda-env-ub_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
