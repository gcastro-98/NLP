{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae8c621",
   "metadata": {},
   "source": [
    "# NLP Task 1: Artur Xarles & Enric Azuara - Train notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cd0b1",
   "metadata": {},
   "source": [
    "Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e612778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b68fa6",
   "metadata": {},
   "source": [
    "Import all necessary functions and classes from $utils.py$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce35a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30ecca",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb68e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"quora_train_data.csv\")\n",
    "A_df, test_df = sklearn.model_selection.train_test_split(train_df, test_size=0.05, random_state=123)\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(A_df, test_size=0.05, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2737124",
   "metadata": {},
   "source": [
    "### Obtain feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af59fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get train data as list of strings \n",
    "q1_list_train = cast_list_as_strings(list(train_df.question1))\n",
    "q2_list_train = cast_list_as_strings(list(train_df.question2))\n",
    "full_list_train = q1_list_train + q2_list_train\n",
    "\n",
    "#Get validation data as list of strings\n",
    "q1_list_val = cast_list_as_strings(list(val_df.question1))\n",
    "q2_list_val = cast_list_as_strings(list(val_df.question2))\n",
    "\n",
    "#Get test data as list of strings\n",
    "q1_list_test = cast_list_as_strings(list(test_df.question1))\n",
    "q2_list_test = cast_list_as_strings(list(test_df.question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d2fef",
   "metadata": {},
   "source": [
    "Get count vectorizer feature vectors and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba251563",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 583794/583794 [00:04<00:00, 117145.68it/s]\n",
      " 15%|██████████▋                                                              | 42713/291897 [00:15<01:33, 2671.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4960/3669324945.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mCountVectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_list_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mq1_train_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1_list_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mq2_train_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq2_list_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mq1_val_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1_list_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\My Drive\\zMaster\\NLP\\Task 1\\deliverable_1\\utils.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                         \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#index representing number of the word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                         \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Column of the word (index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Number of times word appears in sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CountVectorizer = count_vectorizer(full_list_train)\n",
    "CountVectorizer.fit()\n",
    "q1_train_count = CountVectorizer.transform(q1_list_train)\n",
    "q2_train_count = CountVectorizer.transform(q2_list_train)\n",
    "q1_val_count = CountVectorizer.transform(q1_list_val)\n",
    "q2_val_count = CountVectorizer.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/CV_train_q1.npz', q1_train_count)\n",
    "scipy.sparse.save_npz('./data/CV_train_q2.npz', q2_train_count)\n",
    "scipy.sparse.save_npz('./data/CV_val_q1.npz', q1_val_count)\n",
    "scipy.sparse.save_npz('./data/CV_val_q2.npz', q2_val_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e8fe0",
   "metadata": {},
   "source": [
    "Get TF-IDF feature vectors and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = tf_idf(full_list_train)\n",
    "TFIDF.fit()\n",
    "q1_train_tfidf = TFIDF.transform(q1_list_train)\n",
    "q2_train_tfidf = TFIDF.transform(q2_list_train)\n",
    "q1_val_tfidf = TFIDF.transform(q1_list_val)\n",
    "q2_val_tfidf = TFIDF.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b83713",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/tfidf_train_q1.npz', q1_train_tfidf)\n",
    "scipy.sparse.save_npz('./data/tfidf_train_q2.npz', q2_train_tfidf)\n",
    "scipy.sparse.save_npz('./data/tfidf_val_q1.npz', q1_val_tfidf)\n",
    "scipy.sparse.save_npz('./data/tfidf_val_q2.npz', q2_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3208f",
   "metadata": {},
   "source": [
    "Get count vectorizer with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6d6062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Enric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fb2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 583794/583794 [00:26<00:00, 22123.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:35<00:00, 3049.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:34<00:00, 3103.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2920.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2894.61it/s]\n"
     ]
    }
   ],
   "source": [
    "CountVectorizer = count_vectorizer(full_list_train, lemmatization = True)\n",
    "CountVectorizer.fit()\n",
    "q1_train_count_lem = CountVectorizer.transform(q1_list_train)\n",
    "q2_train_count_lem = CountVectorizer.transform(q2_list_train)\n",
    "q1_val_count_lem = CountVectorizer.transform(q1_list_val)\n",
    "q2_val_count_lem = CountVectorizer.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46c9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/CV_lem_train_q1.npz', q1_train_count_lem)\n",
    "scipy.sparse.save_npz('./data/CV_lem_train_q2.npz', q2_train_count_lem)\n",
    "scipy.sparse.save_npz('./data/CV_lem_val_q1.npz', q1_val_count_lem)\n",
    "scipy.sparse.save_npz('./data/CV_lem_val_q2.npz', q2_val_count_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779d4e0",
   "metadata": {},
   "source": [
    "Get TF-IDF with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2016a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 583794/583794 [00:45<00:00, 12769.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:32<00:00, 3155.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:35<00:00, 3049.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2844.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2934.62it/s]\n"
     ]
    }
   ],
   "source": [
    "TFIDF = tf_idf(full_list_train, lemmatization = True)\n",
    "TFIDF.fit()\n",
    "q1_train_tfidf_lem = TFIDF.transform(q1_list_train)\n",
    "q2_train_tfidf_lem = TFIDF.transform(q2_list_train)\n",
    "q1_val_tfidf_lem = TFIDF.transform(q1_list_val)\n",
    "q2_val_tfidf_lem = TFIDF.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2d293fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/tfidf_lem_train_q1.npz', q1_train_tfidf_lem)\n",
    "scipy.sparse.save_npz('./data/tfidf_lem_train_q2.npz', q2_train_tfidf_lem)\n",
    "scipy.sparse.save_npz('./data/tfidf_lem_val_q1.npz', q1_val_tfidf_lem)\n",
    "scipy.sparse.save_npz('./data/tfidf_lem_val_q2.npz', q2_val_tfidf_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad2de2",
   "metadata": {},
   "source": [
    "Get count vectorizer with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae771e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 583794/583794 [01:00<00:00, 9692.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:30<00:00, 3230.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:28<00:00, 3283.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:04<00:00, 3177.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:04<00:00, 3160.44it/s]\n"
     ]
    }
   ],
   "source": [
    "CountVectorizer = count_vectorizer(full_list_train, stemming = True)\n",
    "CountVectorizer.fit()\n",
    "q1_train_count_stem = CountVectorizer.transform(q1_list_train)\n",
    "q2_train_count_stem = CountVectorizer.transform(q2_list_train)\n",
    "q1_val_count_stem = CountVectorizer.transform(q1_list_val)\n",
    "q2_val_count_stem = CountVectorizer.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c5d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/CV_stem_train_q1.npz', q1_train_count_stem)\n",
    "scipy.sparse.save_npz('./data/CV_stem_train_q2.npz', q2_train_count_stem)\n",
    "scipy.sparse.save_npz('./data/CV_stem_val_q1.npz', q1_val_count_stem)\n",
    "scipy.sparse.save_npz('./data/CV_stem_val_q2.npz', q2_val_count_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5ee12",
   "metadata": {},
   "source": [
    "Get TF-IDF with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bee3f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 583794/583794 [02:02<00:00, 4780.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:32<00:00, 3141.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 291897/291897 [01:32<00:00, 3150.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2914.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15363/15363 [00:05<00:00, 2982.03it/s]\n"
     ]
    }
   ],
   "source": [
    "TFIDF = tf_idf(full_list_train, stemming = True)\n",
    "TFIDF.fit()\n",
    "q1_train_tfidf_stem = TFIDF.transform(q1_list_train)\n",
    "q2_train_tfidf_stem = TFIDF.transform(q2_list_train)\n",
    "q1_val_tfidf_stem = TFIDF.transform(q1_list_val)\n",
    "q2_val_tfidf_stem = TFIDF.transform(q2_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8491656",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/tfidf_stem_train_q1.npz', q1_train_tfidf_stem)\n",
    "scipy.sparse.save_npz('./data/tfidf_stem_train_q2.npz', q2_train_tfidf_stem)\n",
    "scipy.sparse.save_npz('./data/tfidf_stem_val_q1.npz', q1_val_tfidf_stem)\n",
    "scipy.sparse.save_npz('./data/tfidf_stem_val_q2.npz', q2_val_tfidf_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a401f",
   "metadata": {},
   "source": [
    "Get extra features from questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "824515af",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_train = get_sim_feature(train_df['question1'].apply(lambda x: str(x).split(' ')),\n",
    "                    train_df['question2'].apply(lambda x: str(x).split(' ')))\n",
    "extra_features_val = get_sim_feature(val_df['question1'].apply(lambda x: str(x).split(' ')),\n",
    "                         val_df['question2'].apply(lambda x: str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba441a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/extra_features_train.npy', extra_features_train)\n",
    "np.save('./data/extra_features_val.npy', extra_features_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f0dd2",
   "metadata": {},
   "source": [
    "Read all feature vectors from stored npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe50250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CV\n",
    "q1_train_count = scipy.sparse.load_npz('./data/CV_train_q1.npz')\n",
    "q2_train_count = scipy.sparse.load_npz('./data/CV_train_q2.npz')\n",
    "q1_val_count = scipy.sparse.load_npz('./data/CV_val_q1.npz')\n",
    "q2_val_count = scipy.sparse.load_npz('./data/CV_val_q2.npz')\n",
    "#TF-IDF\n",
    "q1_train_tfidf = scipy.sparse.load_npz('./data/tfidf_train_q1.npz')\n",
    "q2_train_tfidf = scipy.sparse.load_npz('./data/tfidf_train_q2.npz')\n",
    "q1_val_tfidf = scipy.sparse.load_npz('./data/tfidf_val_q1.npz')\n",
    "q2_val_tfidf = scipy.sparse.load_npz('./data/tfidf_val_q2.npz')\n",
    "#CV lemmatization\n",
    "q1_train_count_lem = scipy.sparse.load_npz('./data/CV_lem_train_q1.npz')\n",
    "q2_train_count_lem = scipy.sparse.load_npz('./data/CV_lem_train_q2.npz')\n",
    "q1_val_count_lem = scipy.sparse.load_npz('./data/CV_lem_val_q1.npz')\n",
    "q2_val_count_lem = scipy.sparse.load_npz('./data/CV_lem_val_q2.npz')\n",
    "#TF-IDF lemmatization\n",
    "q1_train_tfidf_lem = scipy.sparse.load_npz('./data/tfidf_lem_train_q1.npz')\n",
    "q2_train_tfidf_lem = scipy.sparse.load_npz('./data/tfidf_lem_train_q2.npz')\n",
    "q1_val_tfidf_lem = scipy.sparse.load_npz('./data/tfidf_lem_val_q1.npz')\n",
    "q2_val_tfidf_lem = scipy.sparse.load_npz('./data/tfidf_lem_val_q2.npz')\n",
    "#CV stemming\n",
    "q1_train_count_stem = scipy.sparse.load_npz('./data/CV_stem_train_q1.npz')\n",
    "q2_train_count_stem = scipy.sparse.load_npz('./data/CV_stem_train_q2.npz')\n",
    "q1_val_count_stem = scipy.sparse.load_npz('./data/CV_stem_val_q1.npz')\n",
    "q2_val_count_stem = scipy.sparse.load_npz('./data/CV_stem_val_q2.npz')\n",
    "#TF-IDF stemming\n",
    "q1_train_tfidf_stem = scipy.sparse.load_npz('./data/tfidf_stem_train_q1.npz')\n",
    "q2_train_tfidf_stem = scipy.sparse.load_npz('./data/tfidf_stem_train_q2.npz')\n",
    "q1_val_tfidf_stem = scipy.sparse.load_npz('./data/tfidf_stem_val_q1.npz')\n",
    "q2_val_tfidf_stem = scipy.sparse.load_npz('./data/tfidf_stem_val_q2.npz')\n",
    "#Extra features\n",
    "extra_features_train = np.load('./data/extra_features_train.npy')\n",
    "extra_features_val = np.load('./data/extra_features_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cb246",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4b752",
   "metadata": {},
   "source": [
    "First, we train a basic logistic regression model with the Count vectorizer vectors and TF-IDF vectors using different merging strategies (horitzontal stacking, difference between vectors, similarity between vectors and another metric). We use accuracy and logloss to evaluate the models in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "168c8821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Methods using Count Vectorizer features:\n",
      "---------------------------------\n",
      "**** \n",
      "Stack features \n",
      "****\n",
      "logloss validation: 0.5176230416122677\n",
      "Accuracy validation: 0.7490073553342446\n",
      "**** \n",
      "Difference of features \n",
      "****\n",
      "logloss validation: 0.460220255923132\n",
      "Accuracy validation: 0.7715940896960229\n",
      "**** \n",
      "Similarity of features \n",
      "****\n",
      "logloss validation: 0.5940441277862787\n",
      "Accuracy validation: 0.6416064570721864\n",
      "**** \n",
      "Different/product features \n",
      "****\n",
      "logloss validation: 0.4576782926453907\n",
      "Accuracy validation: 0.7715940896960229\n",
      "\n",
      "---------------------------------\n",
      "Methods using TF-IDF features:\n",
      "---------------------------------\n",
      "**** \n",
      "Stack features \n",
      "****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.6723110022656916\n",
      "Accuracy validation: 0.724598060274686\n",
      "**** \n",
      "Difference of features \n",
      "****\n",
      "logloss validation: 0.48640039578895083\n",
      "Accuracy validation: 0.7777126863242856\n",
      "**** \n",
      "Similarity of features \n",
      "****\n",
      "logloss validation: 0.5805705852543142\n",
      "Accuracy validation: 0.6506541691075962\n",
      "**** \n",
      "Different/product features \n",
      "****\n",
      "logloss validation: 0.7133509370738693\n",
      "Accuracy validation: 0.7513506476599623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/log_tfidf_difprod.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Count Vectorizer\n",
    "'''\n",
    "print('---------------------------------\\nMethods using Count Vectorizer features:\\n---------------------------------')\n",
    "\n",
    "print('**** \\nStack features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = stack_features(q1_train_count, q2_train_count)\n",
    "features_val = stack_features(q1_val_count, q2_val_count)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_CV_stack.pkl')\n",
    "\n",
    "print('**** \\nDifference of features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = difference(q1_train_count, q2_train_count)\n",
    "features_val = difference(q1_val_count, q2_val_count)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_CV_diff.pkl')\n",
    "\n",
    "print('**** \\nSimilarity of features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = similarity(q1_train_count, q2_train_count)\n",
    "features_val = similarity(q1_val_count, q2_val_count)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_CV_sim.pkl')\n",
    "\n",
    "print('**** \\nDifferent/product features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = different_product(q1_train_count, q2_train_count)\n",
    "features_val = different_product(q1_val_count, q2_val_count)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_CV_difprod.pkl')\n",
    "\n",
    "'''\n",
    "TF-IDF features\n",
    "'''\n",
    "print('\\n---------------------------------\\nMethods using TF-IDF features:\\n---------------------------------')\n",
    "\n",
    "print('**** \\nStack features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = stack_features(q1_train_tfidf, q2_train_tfidf)\n",
    "features_val = stack_features(q1_val_tfidf, q2_val_tfidf)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_tfidf_stack.pkl')\n",
    "\n",
    "print('**** \\nDifference of features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = difference(q1_train_tfidf, q2_train_tfidf)\n",
    "features_val = difference(q1_val_tfidf, q2_val_tfidf)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_tfidf_dif.pkl')\n",
    "\n",
    "print('**** \\nSimilarity of features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = similarity(q1_train_tfidf, q2_train_tfidf)\n",
    "features_val = similarity(q1_val_tfidf, q2_val_tfidf)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_tfidf_sim.pkl')\n",
    "\n",
    "print('**** \\nDifferent/product features \\n****')\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "features_train = different_product(q1_train_tfidf, q2_train_tfidf)\n",
    "features_val = different_product(q1_val_tfidf, q2_val_tfidf)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values, logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))\n",
    "joblib.dump(logistic, './models/log_tfidf_difprod.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55262beb",
   "metadata": {},
   "source": [
    "We can see that the one that gets better results in terms of accuracy is the one using TF-IDF feature vectors and using the absolute difference of the feature vectores as approach. We will try to use both feature vectors (count vectorizer and tfidf) using the difference approach and adding the cosine similarity to see if we improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d75dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.4782245286309448\n",
      "Accuracy validation: 0.782464362429213\n"
     ]
    }
   ],
   "source": [
    "features_train = scipy.sparse.hstack((difference(q1_train_count, q2_train_count),\n",
    "                                      difference(q1_train_tfidf, q2_train_tfidf),\n",
    "                                      similarity(q1_train_count, q2_train_count),\n",
    "                                      similarity(q1_train_tfidf, q2_train_tfidf)))\n",
    "features_val = scipy.sparse.hstack((difference(q1_val_count, q2_val_count),\n",
    "                                    difference(q1_val_tfidf, q2_val_tfidf),\n",
    "                                    similarity(q1_val_count, q2_val_count),\n",
    "                                    similarity(q1_val_tfidf, q2_val_tfidf)))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "joblib.dump(logistic, './models/log_CV_tfidf_mix.pkl')\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075803e",
   "metadata": {},
   "source": [
    "As we can see, the results are a bit better in the validation set. Therefore, we will keep using this structure of features for the following attemps of improving the model. In the next step, we will try to apply lemmatization and stemming when creating the feature vectors to see if they can improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fa985",
   "metadata": {},
   "source": [
    "*Applying lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfedfdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.47654072573649153\n",
      "Accuracy validation: 0.7740024734752327\n"
     ]
    }
   ],
   "source": [
    "features_train = scipy.sparse.hstack((difference(q1_train_count_lem, q2_train_count_lem),\n",
    "                                      difference(q1_train_tfidf_lem, q2_train_tfidf_lem),\n",
    "                                      similarity(q1_train_count_lem, q2_train_count_lem),\n",
    "                                      similarity(q1_train_tfidf_lem, q2_train_tfidf_lem)))\n",
    "features_val = scipy.sparse.hstack((difference(q1_val_count_lem, q2_val_count_lem),\n",
    "                                    difference(q1_val_tfidf_lem, q2_val_tfidf_lem),\n",
    "                                    similarity(q1_val_count_lem, q2_val_count_lem),\n",
    "                                    similarity(q1_val_tfidf_lem, q2_val_tfidf_lem)))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "joblib.dump(logistic, './models/log_CV_tfidf_mix_lem.pkl')\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e9969",
   "metadata": {},
   "source": [
    "*Applying stemming*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64ca444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.530299058365541\n",
      "Accuracy validation: 0.761635097311723\n"
     ]
    }
   ],
   "source": [
    "features_train = scipy.sparse.hstack((different_product(q1_train_count_stem, q2_train_count_stem),\n",
    "                                      different_product(q1_train_tfidf_stem, q2_train_tfidf_stem),\n",
    "                                      similarity(q1_train_count_stem, q2_train_count_stem),\n",
    "                                      similarity(q1_train_tfidf_stem, q2_train_tfidf_stem)))\n",
    "features_val = scipy.sparse.hstack((different_product(q1_val_count_stem, q2_val_count_stem),\n",
    "                                    different_product(q1_val_tfidf_stem, q2_val_tfidf_stem),\n",
    "                                    similarity(q1_val_count_stem, q2_val_count_stem),\n",
    "                                    similarity(q1_val_tfidf_stem, q2_val_tfidf_stem)))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic.fit(features_train, train_df['is_duplicate'].values)\n",
    "joblib.dump(logistic, './models/log_CV_tfidf_mix_stem.pkl')\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            logistic.predict_proba(features_val))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf165f6",
   "metadata": {},
   "source": [
    "We can see that neither lemmatization or stemming improve the results, so we will not use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7224c6f",
   "metadata": {},
   "source": [
    "As a final step, we will try to add the extra features previously computed to see if they can add valuable information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7dca5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.4567309766744149\n",
      "Accuracy validation: 0.7912517086506542\n"
     ]
    }
   ],
   "source": [
    "Variable_X_train = Variable_KeyWords(train_df)\n",
    "features_train_2 = scipy.sparse.hstack((difference(q1_train_count, q2_train_count),\n",
    "                                      difference(q1_train_tfidf, q2_train_tfidf),\n",
    "                                      similarity(q1_train_count, q2_train_count),\n",
    "                                      similarity(q1_train_tfidf, q2_train_tfidf),\n",
    "                                      np.hstack([extra_features_train, Variable_X_train])))\n",
    "\n",
    "Variable_X_val = Variable_KeyWords(val_df)\n",
    "features_val_2 = scipy.sparse.hstack((difference(q1_val_count, q2_val_count),\n",
    "                                    difference(q1_val_tfidf, q2_val_tfidf),\n",
    "                                    similarity(q1_val_count, q2_val_count),\n",
    "                                    similarity(q1_val_tfidf, q2_val_tfidf),\n",
    "                                    np.hstack([extra_features_val, Variable_X_val])))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic.fit(features_train_2, train_df['is_duplicate'].values)\n",
    "joblib.dump(logistic, './models/log_CV_tfidf_extra_mix.pkl')\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            logistic.predict_proba(features_val_2))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val_2)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54717752",
   "metadata": {},
   "source": [
    "The extra features improve a little bit the results so we will keep this feature combination for the models. Finally, using the features selected we will try different models (xgboost with different parameters) to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "66a9e40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:08:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:18:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:02:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\envs\\Algebra\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:25:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=10, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=1400, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=123,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "estimator = XGBClassifier(random_state = 123)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': range(1200, 1600, 200),\n",
    "    'learning_rate': [0.1, 0.05]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = estimator,\n",
    "    param_grid = parameters,\n",
    "    scoring = 'accuracy',\n",
    "    cv = sklearn.model_selection.KFold(2),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#grid_search.fit(features_train_2, train_df['is_duplicate'].values)\n",
    "\n",
    "#print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1782c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss validation: 0.3869561994078014\n",
      "Accuracy validation: 0.8098678643494109\n"
     ]
    }
   ],
   "source": [
    "Best = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=1400, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=123,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "Best.fit(features_train_2, train_df['is_duplicate'].values)\n",
    "joblib.dump(Best, './models/best_model.pkl')\n",
    "print('logloss validation: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            Best.predict_proba(features_val_2))))\n",
    "print('Accuracy validation: ' + str((val_df['is_duplicate'].values == Best.predict(features_val_2)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804db21",
   "metadata": {},
   "source": [
    "As we can see the best xgboost model has a better performance than the logistic one in the evaluation set so we will keep this one as our final model using the previously mentioned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f3296",
   "metadata": {},
   "source": [
    "#### Logistic model trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5226e4",
   "metadata": {},
   "source": [
    "Now we will check changing some parameters of the logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ac4cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss test: 0.44647572263781243\n",
      "Accuracy test: 0.7896895137668424\n"
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123,\n",
    "                                                  penalty=\"l1\")\n",
    "logistic.fit(features_train_2, train_df['is_duplicate'].values)\n",
    "joblib.dump(logistic, './models/logistic_l1.pkl')\n",
    "print('logloss test: ' + str(sklearn.metrics.log_loss(val_df['is_duplicate'].values,\n",
    "                                                            logistic.predict_proba(features_val_2))))\n",
    "print('Accuracy test: ' + str((val_df['is_duplicate'].values == logistic.predict(features_val_2)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89218dd6",
   "metadata": {},
   "source": [
    "We see that changing to the l1 does not improve our results (we had 0.7912)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d26b4",
   "metadata": {},
   "source": [
    "### Train final model with train + validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "442e1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614520/614520 [00:03<00:00, 156985.45it/s]\n",
      "100%|██████████| 307260/307260 [01:28<00:00, 3479.01it/s]\n",
      "100%|██████████| 307260/307260 [01:32<00:00, 3325.09it/s]\n",
      "100%|██████████| 16172/16172 [00:05<00:00, 3027.90it/s]\n",
      "100%|██████████| 16172/16172 [00:05<00:00, 3099.44it/s]\n"
     ]
    }
   ],
   "source": [
    "full_list_all_train = q1_list_train + q2_list_train + q1_list_val + q2_list_val\n",
    "CountVectorizer = count_vectorizer(full_list_all_train)\n",
    "CountVectorizer.fit()\n",
    "q1_all_train_count = CountVectorizer.transform(q1_list_train + q1_list_val)\n",
    "q2_all_train_count = CountVectorizer.transform(q2_list_train + q2_list_val)\n",
    "q1_test_count = CountVectorizer.transform(q1_list_test)\n",
    "q2_test_count = CountVectorizer.transform(q2_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1d442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614520/614520 [00:05<00:00, 109892.80it/s]\n",
      "100%|██████████| 307260/307260 [01:30<00:00, 3401.53it/s]\n",
      "100%|██████████| 307260/307260 [01:34<00:00, 3260.73it/s]\n",
      "100%|██████████| 16172/16172 [00:05<00:00, 2915.98it/s]\n",
      "100%|██████████| 16172/16172 [00:05<00:00, 3113.60it/s]\n"
     ]
    }
   ],
   "source": [
    "TFIDF = tf_idf(full_list_all_train)\n",
    "TFIDF.fit()\n",
    "q1_all_train_tfidf = TFIDF.transform(q1_list_train + q1_list_val)\n",
    "q2_all_train_tfidf = TFIDF.transform(q2_list_train + q2_list_val)\n",
    "q1_test_tfidf = TFIDF.transform(q1_list_test)\n",
    "q2_test_tfidf = TFIDF.transform(q2_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa606c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_df = pd.concat((train_df, val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c906743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_all_train = get_sim_feature(all_train_df['question1'].apply(lambda x: str(x).split(' ')),\n",
    "                    all_train_df['question2'].apply(lambda x: str(x).split(' ')))\n",
    "extra_features_test = get_sim_feature(test_df['question1'].apply(lambda x: str(x).split(' ')),\n",
    "                    test_df['question2'].apply(lambda x: str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe314e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable_X_all_train = Variable_KeyWords(all_train_df)\n",
    "final_features_train = scipy.sparse.hstack((difference(q1_all_train_count, q2_all_train_count),\n",
    "                                      difference(q1_all_train_tfidf, q2_all_train_tfidf),\n",
    "                                      similarity(q1_all_train_count, q2_all_train_count),\n",
    "                                      similarity(q1_all_train_tfidf, q2_all_train_tfidf),\n",
    "                                      np.hstack([extra_features_all_train, Variable_X_all_train])))\n",
    "Variable_X_test = Variable_KeyWords(test_df)\n",
    "final_features_test = scipy.sparse.hstack((difference(q1_test_count, q2_test_count),\n",
    "                                    difference(q1_test_tfidf, q2_test_tfidf),\n",
    "                                    similarity(q1_test_count, q2_test_count),\n",
    "                                    similarity(q1_test_tfidf, q2_test_tfidf),\n",
    "                                    np.hstack([extra_features_test, Variable_X_test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('./data/final_features_train.npz', final_features_train)\n",
    "scipy.sparse.save_npz('./data/final_features_test.npz', final_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878b748",
   "metadata": {},
   "source": [
    "Once stored the whole final features, we read them and train the final logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73ed9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_train = scipy.sparse.load_npz('./data/final_features_train.npz')\n",
    "final_features_test = scipy.sparse.load_npz('./data/final_features_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a89695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss test: 0.3820196153925488\n",
      "Accuracy test: 0.8136284936928023\n"
     ]
    }
   ],
   "source": [
    "Best = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=1400, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=123,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "Best.fit(final_features_train, all_train_df['is_duplicate'].values)\n",
    "joblib.dump(Best, './models/final_model.pkl')\n",
    "print('logloss test: ' + str(sklearn.metrics.log_loss(test_df['is_duplicate'].values,\n",
    "                                                            Best.predict_proba(final_features_test))))\n",
    "print('Accuracy test: ' + str((test_df['is_duplicate'].values == Best.predict(final_features_test)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea681a",
   "metadata": {},
   "source": [
    "We can check the that we have a logloss of $0.382$ and an accuracy of $0.814$ in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee48cb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8995489030628392\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, _ = sklearn.metrics.roc_curve(test_df['is_duplicate'].values,  Best.predict_proba(final_features_test)[:, 1])\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "print('AUC: ' + str(sklearn.metrics.roc_auc_score(test_df['is_duplicate'].values, Best.predict_proba(final_features_test)[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cc9e1",
   "metadata": {},
   "source": [
    "We obtain an area under the curve of 0.9, which is a great result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
